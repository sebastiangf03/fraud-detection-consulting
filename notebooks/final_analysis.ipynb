
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Carga del dataset\n",
    "# Asegúrate de que el archivo 'creditcard.csv' esté en la misma carpeta\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# 2. Validación de Integridad\n",
    "print(\"--- VALIDACIÓN DE DATOS ---\")\n",
    "print(f\"1. Total de registros: {df.shape[0]}\")\n",
    "print(f\"2. Total de columnas: {df.shape[1]}\")\n",
    "print(f\"3. ¿Hay valores nulos?: {df.isnull().sum().sum()}\")\n",
    "print(f\"4. ¿Hay registros duplicados?: {df.duplicated().sum()}\")\n",
    "\n",
    "# 3. Tipos de datos\n",
    "# Es vital saber si el dinero ('Amount') o el tiempo ('Time') son números reales\n",
    "print(\"\\n--- TIPOS DE DATOS ---\")\n",
    "print(df[['Time', 'Amount', 'Class']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo del desbalance \n",
    "conteo = df['Class'].value_counts(normalize=True) * 100\n",
    "print(f\"\\n--- DISTRIBUCIÓN DEL FRAUDE ---\")\n",
    "print(f\"Transacciones Normales (0): {conteo[0]:.2f}%\")\n",
    "print(f\"Transacciones Fraude (1): {conteo[1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dperdido=df[df['Class'] == 1]['Amount'].sum()\n",
    "dperdido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Eliminamos duplicados como sugeriste\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# 2. Inicializamos el escalador\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "# 3. Escalamos 'Amount' y 'Time' (las únicas que no lo están)\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "# 4. Quitamos las columnas originales para no confundir al modelo\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "# 5. Reorganizamos para tener las escaladas al principio (por orden)\n",
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "print(\"Datos escalados y listos. Primeras 5 filas:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35110075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Barajamos los datos antes de recortar\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# 2. Separamos fraude y normales\n",
    "fraude_df = df.loc[df['Class'] == 1]\n",
    "normal_df = df.loc[df['Class'] == 0][:473] # Tomamos solo 473 normales\n",
    "\n",
    "# 3. Concatenamos para crear el dataset equilibrado\n",
    "normal_dist_df = pd.concat([fraude_df, normal_df])\n",
    "\n",
    "# 4. Volvemos a barajar para que el modelo no aprenda el orden\n",
    "new_df = normal_dist_df.sample(frac=1, random_state=42)\n",
    "\n",
    "print(f\"Nuevo dataset equilibrado: {new_df.shape}\")\n",
    "print(new_df['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0beefe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuramos el lienzo\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 20))\n",
    "\n",
    "# 1. Matriz de Correlación - Dataset Original (Desequilibrado)\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Matriz de Correlación (Desequilibrada) \\n No la uses para sacar conclusiones\", fontsize=18)\n",
    "\n",
    "# 2. Matriz de Correlación - Nuevo Dataset (Equilibrado)\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('Matriz de Correlación (Equilibrada) \\n ¡Aquí está la información real!', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a mirar las correlaciones para ver cual tiene mas peso en el modelo\n",
    "# # Ordenados de menor a mayor\n",
    "print(\"Variables con mayor impacto negativo:\")\n",
    "print(new_df.corr()['Class'].sort_values(ascending=True).head(5))\n",
    "\n",
    "print(\"\\nVariables con mayor impacto positivo:\")\n",
    "print(new_df.corr()['Class'].sort_values(ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos V14 (la que tiene correlación negativa más fuerte)\n",
    "v14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n",
    "q25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
    "v14_iqr = q75 - q25\n",
    "\n",
    "# Calculamos los límites (Corte de Tukey)\n",
    "v14_cut_off = v14_iqr * 1.5\n",
    "v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\n",
    "\n",
    "print(f\"Límite inferior para V14: {v14_lower}\")\n",
    "print(f\"Límite superior para V14: {v14_upper}\")\n",
    "\n",
    "# Eliminamos los outliers del dataset\n",
    "outliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\n",
    "print(f'Outliers identificados en V14: {len(outliers)}')\n",
    "\n",
    "new_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\n",
    "print('---' * 10)\n",
    "print(f'Registros restantes tras limpiar V14: {len(new_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d90f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# 1. Separamos las variables (X) de la etiqueta (y)\n",
    "X = new_df.drop('Class', axis=1)\n",
    "y = new_df['Class']\n",
    "\n",
    "# 2. Dividimos el dataset (80% entrenamiento, 20% prueba)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Entrenamos el modelo\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 4. Hacemos las predicciones\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"¡Modelo entrenado y predicciones generadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la Matriz de Confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicción (IA)')\n",
    "plt.ylabel('Realidad (Banco)')\n",
    "plt.title('Matriz de Confusión: ¿Qué tan buena es nuestra IA?')\n",
    "plt.show()\n",
    "\n",
    "# Resumen de métricas\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0b70a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Usamos el dataset ORIGINAL \n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# 2. División Crítica: Mantenemos el test con datos reales y desbalanceados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Aplicamos SMOTE solo al entrenamiento\n",
    "print(f\"Antes de SMOTE: {sum(y_train==1)} fraudes\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(f\"Después de SMOTE: {sum(y_train_res==1)} fraudes (¡Equilibrado!)\")\n",
    "\n",
    "# 4. Entrenamos la misma Regresión Logística\n",
    "log_reg_smote = LogisticRegression(max_iter=1000)\n",
    "log_reg_smote.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 5. Predicción sobre los datos reales de TEST\n",
    "y_pred_smote = log_reg_smote.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d9a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Generamos la matriz de confusión para las predicciones con SMOTE\n",
    "cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
    "\n",
    "# 2. Visualización\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cm_smote, annot=True, fmt='d', cmap='Greens', cbar=True)\n",
    "plt.xlabel('Predicción (IA con SMOTE)')\n",
    "plt.ylabel('Realidad (Banco)')\n",
    "plt.title('Matriz de Confusión: Modelo con Entrenamiento SMOTE')\n",
    "plt.show()\n",
    "\n",
    "# 3. Informe detallado\n",
    "print(\"--- INFORME DE RENDIMIENTO (SMOTE) ---\")\n",
    "print(classification_report(y_test, y_pred_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6cf6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Inicializar el modelo (con 100 árboles para empezar)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 2. Entrenar con los datos de SMOTE (los mismos que usamos antes)\n",
    "print(\"Entrenando el Bosque Aleatorio... Esto puede tardar un poco más.\")\n",
    "rf_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# 3. Predicciones\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 4. Evaluación\n",
    "print(\"\\n--- INFORME DE RENDIMIENTO: RANDOM FOREST + SMOTE ---\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# 5. Matriz de Confusión\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Purples', cbar=True)\n",
    "plt.title('Matriz de Confusión: Random Forest + SMOTE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
